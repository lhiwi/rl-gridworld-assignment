{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72e4aa18",
   "metadata": {},
   "source": [
    "# Value Iteration (5x5 Gridworld)\n",
    "\n",
    "This notebook implements **Value Iteration** to solve a fully observable gridworld MDP.\n",
    "\n",
    "**Environment**\n",
    "- Grid: 5x5\n",
    "- Actions: Up, Down, Left, Right\n",
    "- Reward: +10 for reaching the goal, -1 otherwise\n",
    "- Terminal: goal state\n",
    "- Discount: γ = 0.9\n",
    "\n",
    "**Output**\n",
    "- Optimal value function \\(V^*(s)\\)\n",
    "- Optimal policy \\(\\pi^*(s)\\) after convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3b9c3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports + Constants \n",
    "import numpy as np\n",
    "from typing import Tuple, Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58e3efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDP Setup \n",
    "# Grid size required by the question\n",
    "GRID_SIZE = 5\n",
    "\n",
    "# Discount factor required by the question\n",
    "GAMMA = 0.9\n",
    "\n",
    "# Convergence tolerance (small threshold for stopping condition)\n",
    "THETA = 1e-6\n",
    "\n",
    "# Define actions and deterministic movement deltas\n",
    "ACTIONS = {\n",
    "    \"U\": (-1, 0),\n",
    "    \"D\": ( 1, 0),\n",
    "    \"L\": ( 0,-1),\n",
    "    \"R\": ( 0, 1),\n",
    "}\n",
    "\n",
    "# Choose a goal state (standard: bottom-right corner)\n",
    "GOAL_STATE = (4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15c83bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers + Transition Function \n",
    "def in_bounds(r: int, c: int) -> bool:\n",
    "    # Checks whether a cell is inside the 5x5 grid\n",
    "    return 0 <= r < GRID_SIZE and 0 <= c < GRID_SIZE\n",
    "\n",
    "def all_states() -> List[Tuple[int,int]]:\n",
    "    # Enumerates every state (r,c) in the grid\n",
    "    return [(r, c) for r in range(GRID_SIZE) for c in range(GRID_SIZE)]\n",
    "\n",
    "def step(state: Tuple[int,int], action: str) -> Tuple[Tuple[int,int], float, bool]:\n",
    "    \"\"\"\n",
    "    Deterministic transition function.\n",
    "    - If state is terminal (goal), remain there with 0 reward.\n",
    "    - Otherwise move; if off-grid, stay in place.\n",
    "    - Reward: +10 if next state is goal, else -1.\n",
    "    - done=True if next state is goal.\n",
    "    \"\"\"\n",
    "    if state == GOAL_STATE:\n",
    "        return state, 0.0, True  # terminal absorbing\n",
    "    \n",
    "    dr, dc = ACTIONS[action]\n",
    "    nr, nc = state[0] + dr, state[1] + dc\n",
    "\n",
    "    # If move hits wall, agent stays in the same state\n",
    "    if not in_bounds(nr, nc):\n",
    "        nr, nc = state\n",
    "    \n",
    "    next_state = (nr, nc)\n",
    "\n",
    "    # Reward rule from the question statement\n",
    "    reward = 10.0 if next_state == GOAL_STATE else -1.0\n",
    "\n",
    "    # Terminal condition\n",
    "    done = (next_state == GOAL_STATE)\n",
    "    return next_state, reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8009086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration \n",
    "def value_iteration() -> Tuple[np.ndarray, Dict[Tuple[int,int], str]]:\n",
    "    \"\"\"\n",
    "    Value Iteration:\n",
    "        V_{k+1}(s) = max_a [ r(s,a,s') + gamma * V_k(s') ]\n",
    "    Stops when max state change < THETA.\n",
    "    \"\"\"\n",
    "    V = np.zeros((GRID_SIZE, GRID_SIZE), dtype=float)  # initialize V(s)=0\n",
    "\n",
    "    while True:\n",
    "        delta = 0.0  # max change in this iteration\n",
    "\n",
    "        for s in all_states():\n",
    "            if s == GOAL_STATE:\n",
    "                continue  # keep terminal fixed\n",
    "            \n",
    "            old_v = V[s]  # store previous value\n",
    "\n",
    "            # Compute one-step lookahead values for each action\n",
    "            action_values = []\n",
    "            for a in ACTIONS.keys():\n",
    "                s_next, r, done = step(s, a)\n",
    "                future = 0.0 if done else V[s_next]  # no future beyond terminal\n",
    "                action_values.append(r + GAMMA * future)\n",
    "\n",
    "            # Bellman optimality backup\n",
    "            V[s] = max(action_values)\n",
    "\n",
    "            # Update max change for convergence test\n",
    "            delta = max(delta, abs(old_v - V[s]))\n",
    "\n",
    "        if delta < THETA:\n",
    "            break\n",
    "\n",
    "    # Extract optimal greedy policy from converged V\n",
    "    policy = {}\n",
    "    for s in all_states():\n",
    "        if s == GOAL_STATE:\n",
    "            policy[s] = \"G\"\n",
    "            continue\n",
    "        \n",
    "        best_a = None\n",
    "        best_q = -float(\"inf\")\n",
    "\n",
    "        for a in ACTIONS.keys():\n",
    "            s_next, r, done = step(s, a)\n",
    "            future = 0.0 if done else V[s_next]\n",
    "            q_sa = r + GAMMA * future\n",
    "\n",
    "            if q_sa > best_q:\n",
    "                best_q = q_sa\n",
    "                best_a = a\n",
    "\n",
    "        policy[s] = best_a\n",
    "\n",
    "    return V, policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f95fb187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function V* (rounded):\n",
      "[[-0.434  0.629  1.81   3.122  4.58 ]\n",
      " [ 0.629  1.81   3.122  4.58   6.2  ]\n",
      " [ 1.81   3.122  4.58   6.2    8.   ]\n",
      " [ 3.122  4.58   6.2    8.    10.   ]\n",
      " [ 4.58   6.2    8.    10.     0.   ]]\n",
      "\n",
      "Optimal Policy (Value Iteration):\n",
      "↓ ↓ ↓ ↓ ↓\n",
      "↓ ↓ ↓ ↓ ↓\n",
      "↓ ↓ ↓ ↓ ↓\n",
      "↓ ↓ ↓ ↓ ↓\n",
      "→ → → → G\n"
     ]
    }
   ],
   "source": [
    "# Print Results \n",
    "def print_policy(policy: Dict[Tuple[int,int], str]) -> None:\n",
    "    arrow = {\"U\":\"↑\", \"D\":\"↓\", \"L\":\"←\", \"R\":\"→\", \"G\":\"G\"}\n",
    "    print(\"\\nOptimal Policy (Value Iteration):\")\n",
    "    for r in range(GRID_SIZE):\n",
    "        print(\" \".join(arrow[policy[(r,c)]] for c in range(GRID_SIZE)))\n",
    "\n",
    "V_star, pi_star = value_iteration()\n",
    "\n",
    "print(\"Optimal Value Function V* (rounded):\")\n",
    "print(np.round(V_star, 3))\n",
    "\n",
    "print_policy(pi_star)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV Labs (Py3.13 venv)",
   "language": "python",
   "name": "cv-labs-py313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
